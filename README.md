# Deep-Learning_Eye-For-Blind

## Introduction:
The project, "Eye for Blind," aims to create a deep learning model that can explain the content of an image in the form of speech through caption generation with the attention mechanism on the Flickr8K data set


## Goal:
It aims to use text to speech conversion in order to showcase our result in an audio format, thus, allowing us to  recognize the objects and explain them accordingly in an audiblemanner.


## Future work: 
create an application to help blind people explain the pictures accordingly in an audible manner.


## Dataset:
  - 8091 Images
  - 40455 Captions
## Dataset sourec:
from Kaggle website [[Kaggle]](https://www.kaggle.com/santhraul/eye-for-blind)


## Algorithms:
- Inception-v3Â model 
- CCN Model.
- Attention Model. 
- RNN Model. 
- Greedy Search
- Beam Search
- Gtts

## Tools:
### Softwares:
<hr>

1. VScode
2. mp3
3. Trello
4. Jupyter
5. Github
6. PowerPoint
7. Zoom

### Languages & Libarry
<hr>

   - Python
   - Pandas
   - numpy
   - seaborn
   - plotly
   - sklearn
   - PIL
   - tqdm
   - Adam
   - InceptionV3


## Team Members
 - [@Shaima Alzahrani](https://github.com/shaimaalabedi)
 - [@Raghad Alnasser](https://github.com/EngrRaghad)
